# Your Phone Can Spot Fashion

### Abstract
This model introduces a comprehensive pipeline for fashion product discovery in short-form videos, exemplified by platforms such as Instagram Reels and TikTok. It integrates both visual and textual cues by extracting frames for object detection while leveraging video captions and hashtags to infer the underlying vibe. First, visual-semantic embeddings are generated using the Contrastive Language-Image Pre-training (CLIP) model, enabling similarity search against a curated product catalog through Facebook AI Similarity Search (FAISS). The detected items are then matched as Exact Match, Similar Match, or No Match to improve retrieval precision, with performance evaluated through detailed visualizations including confidence score distributions, match type breakdowns, and product type frequency heatmaps. To address ethical concerns regarding user privacy and data protection, all video and product data are processed and stored locally without external transmission. This work lays essential groundwork for future research in automated tagging, vibe classification, and intelligent product discovery in video-driven e-commerce.
